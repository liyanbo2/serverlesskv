{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "model_path = \"/dataset/crosspipe/OriginModel/Llama-2-7b-chat-hf/\"\n",
    " \n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, device_map=device)\n",
    "for use_cache in ( False,True):\n",
    "  times = []\n",
    "  for _ in range(1):  # measuring 10 generations\n",
    "      start = time.time()\n",
    "      input = tokenizer(\"What is KV caching?\", return_tensors=\"pt\").to(device)\n",
    "      outputs = model.generate(**input, use_cache=use_cache, max_new_tokens=1000, temperature=0.00001)\n",
    "      times.append(time.time() - start)\n",
    "      print(outputs)\n",
    "  print(f\"{'With' if use_cache else 'Without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([[    1,  1724,   338,   476, 29963, 22488, 29973,    13,    13, 29968,\n",
    "         29963, 22488,   338,   263, 11043,  1304,   297,  7047,  5849,   304,\n",
    "         11157,   278,  4180,   310,  8324,   491, 15446, 13672, 20592,   848,\n",
    "           297,   263,  7090, 29889,   476, 29963, 15028,   363,   376,  1989,\n",
    "         29899,  1767,  1699,   607, 14637,   304,   278,  1134,   310,   848,\n",
    "          6087,   297,   278,  7090, 29889,    13,    13,   797,   263,   476,\n",
    "         29963,  7090, 29892,   848,   338,  6087,   408,   263,  5101,   310,\n",
    "           263,  1820,   322,   263,   995, 29889,   450,  1820,   338,  1304,\n",
    "           304, 12439,   278,   848, 29892,   322,   278,   995,   338,   278,\n",
    "           848,  3528, 29889,   450,  7090,   338, 12234,  8762,   408,   263,\n",
    "           848,  3829, 29892,  1316,   408,   263,  6608,  1591,   470,   263,\n",
    "          5447, 29892,   393,  6511,   363,  8543,  1106, 14340,   322, 11217,\n",
    "           310,   278,  6087,   848, 29889,    13,    13, 29968, 29963, 22488,\n",
    "           338, 15574,  1304,   297,  1856,  8324,   304, 11157,   278,  4180,\n",
    "           310,  2566,  9365, 29889,  8669,   310,  2346,   292,   278,  2566,\n",
    "           363,  1432,  2009, 29892,   278,  2280,   508,   671,   278,   476,\n",
    "         29963,  7090,   304,  3787, 13672, 20592,   848, 29892,  1316,   408,\n",
    "          1404,  4867,  2472,   470, 13672, 20592,  6515, 29889,   910, 26830,\n",
    "           278,  1353,   310,  2566,  9365, 29892,   607,   508, 11157,   278,\n",
    "          2280, 29915, 29879,  4180,   322,  8716,  3097, 29889,    13,    13,\n",
    "         29968, 29963, 22488,   508,   884,   367,  1304,   297,   916, 10161,\n",
    "           310,  7047,  5849, 29892,  1316,   408, 29901,    13,    13, 29930,\n",
    "           315,  9733, 13672, 20592,   848,   297,   263,  1856,  2280,   304,\n",
    "         10032,   278,  1353,   310,  2566,  9365, 29889,    13, 29930,   624,\n",
    "          8253, 15562, 29892,  1316,   408,  1404,  5821,  2063,   470,  2740,\n",
    "          4955, 29892,   297,   263,  7090,   304, 11157,   278,  4180,   310,\n",
    "           263,  1856,  2280, 29889,    13, 29930,   315,  9733, 13672, 20592,\n",
    "           848,   297,   263, 13235,  1788,   304, 10032,   278,  2254,   373,\n",
    "           278,  1788,   322, 11157,   967,  8716,  3097, 29889,    13,    13,\n",
    "           797, 15837, 29892,   476, 29963, 22488,   338,   263, 11043,  1304,\n",
    "           304, 11157,   278,  4180,   310,  8324,   491, 15446, 13672, 20592,\n",
    "           848,   297,   263,  7090, 29889,   739,   338, 15574,  1304,   297,\n",
    "          1856,  8324,   304, 10032,   278,  1353,   310,  2566,  9365, 29892,\n",
    "           541,   508,   884,   367,  1304,   297,   916, 10161,   310,  7047,\n",
    "          5849,   304, 11157,  4180,   322,  8716,  3097, 29889,     2]],\n",
    "       device='cuda:2')\n",
    "With KV caching: 18.327 +- 0.0 seconds\n",
    "tensor([[    1,  1724,   338,   476, 29963, 22488, 29973,    13,    13, 29968,\n",
    "         29963, 22488,   338,   263, 11043,  1304,   297,  7047,  5849,   304,\n",
    "         11157,   278,  4180,   310,  8324,   491, 15446, 13672, 20592,   848,\n",
    "           297,   263,  7090, 29889,   476, 29963, 15028,   363,   376,  1989,\n",
    "         29899,  1767,  1699,   607, 14637,   304,   278,  1134,   310,   848,\n",
    "          6087,   297,   278,  7090, 29889,    13,    13,   797,   263,   476,\n",
    "         29963,  7090, 29892,   848,   338,  6087,   408,   263,  5101,   310,\n",
    "           263,  1820,   322,   263,   995, 29889,   450,  1820,   338,  1304,\n",
    "           304, 12439,   278,   848, 29892,   322,   278,   995,   338,   278,\n",
    "           848,  3528, 29889,   450,  7090,   338, 12234,  8762,   408,   263,\n",
    "           848,  3829, 29892,  1316,   408,   263,  6608,  1591,   470,   263,\n",
    "          5447, 29892,   393,  6511,   363,  8543,  1106, 14340,   322, 11217,\n",
    "           310,   278,  6087,   848, 29889,    13,    13, 29968, 29963, 22488,\n",
    "           338, 15574,  1304,   297,  1856,  8324,   304, 11157,   278,  4180,\n",
    "           310,  2566,  9365, 29889,  8669,   310,  2346,   292,   278,  2566,\n",
    "           363,  1432,  2009, 29892,   278,  2280,   508,   671,   278,   476,\n",
    "         29963,  7090,   304,  3787, 13672, 20592,   848, 29892,  1316,   408,\n",
    "          1404,  4867,  2472,   470, 13672, 20592,  6515, 29889,   910, 26830,\n",
    "           278,  1353,   310,  2566,  9365, 29892,   607,   508, 11157,   278,\n",
    "          2280, 29915, 29879,  4180,   322,  8716,  3097, 29889,    13,    13,\n",
    "         29968, 29963, 22488,   508,   884,   367,  1304,   297,   916, 10161,\n",
    "           310,  7047,  5849, 29892,  1316,   408, 29901,    13,    13, 29930,\n",
    "           315,  9733, 13672, 20592,   848,   297,   263,  1856,  2280,   304,\n",
    "         10032,   278,  1353,   310,  2566,  9365, 29889,    13, 29930,   624,\n",
    "          8253, 15562, 29892,  1316,   408,  1404,  5821,  2063,   470,  2740,\n",
    "          4955, 29892,   297,   263,  7090,   304, 11157,   278,  4180,   310,\n",
    "           263,  1856,  2280, 29889,    13, 29930,   315,  9733, 13672, 20592,\n",
    "           848,   297,   263, 13235,  1788,   304, 10032,   278,  2254,   373,\n",
    "           278,  1788,   322, 11157,   967,  8716,  3097, 29889,    13,    13,\n",
    "           797, 15837, 29892,   476, 29963, 22488,   338,   263, 11043,  1304,\n",
    "           304, 11157,   278,  4180,   310,  8324,   491, 15446, 13672, 20592,\n",
    "           848,   297,   263,  7090, 29889,   739,   338, 15574,  1304,   297,\n",
    "          1856,  8324,   304, 10032,   278,  1353,   310,  2566,  9365, 29892,\n",
    "           541,   508,   884,   367,  1304,   297,   916, 10161,   310,  7047,\n",
    "          5849,   304, 11157,  4180,   322,  8716,  3097, 29889,     2]],\n",
    "       device='cuda:2')\n",
    "Without KV caching: 80.208 +- 0.0 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([[    1,  1724,   338,   476, 29963, 22488, 29973,    13,    13, 29968,\n",
    "         29963, 22488,   338,   263, 11043,  1304,   297,  7047,  5849,   304,\n",
    "         11157,   278,  4180,   310,  8324,   491, 15446, 13672, 20592,   848,\n",
    "           297,   263,  7090, 29889,   476, 29963, 15028,   363,   376,  1989,\n",
    "         29899,  1767,  1699,   607, 14637,   304,   278,  1134,   310,   848,\n",
    "          6087,   297,   278,  7090, 29889,    13,    13,   797,   263,   476,\n",
    "         29963,  7090, 29892,   848,   338,  6087,   408,   263,  5101,   310,\n",
    "           263,  1820,   322,   263,   995, 29889,   450,  1820,   338,  1304,\n",
    "           304, 12439,   278,   848, 29892,   322,   278,   995,   338,   278,\n",
    "           848,  3528, 29889,   450,  7090,   338, 12234,  8762,   408,   263,\n",
    "           848,  3829, 29892,  1316,   408,   263,  6608,  1591,   470,   263,\n",
    "          5447, 29892,   393,  6511,   363,  8543,  1106, 14340,   322, 11217,\n",
    "           310,   278,  6087,   848, 29889,    13,    13, 29968, 29963, 22488,\n",
    "           338, 15574,  1304,   297,  1856,  8324,   304, 11157,   278,  4180,\n",
    "           310,  2566,  9365, 29889,  8669,   310,  2346,   292,   278,  2566,\n",
    "           363,  1432,  2009, 29892,   278,  2280,   508,   671,   278,   476,\n",
    "         29963,  7090,   304,  3787, 13672, 20592,   848, 29892,  1316,   408,\n",
    "          1404,  4867,  2472,   470, 13672, 20592,  6515, 29889,   910, 26830,\n",
    "           278,  1353,   310,  2566,  9365, 29892,   607,   508, 11157,   278,\n",
    "          2280, 29915, 29879,  4180,   322,  8716,  3097, 29889,    13,    13,\n",
    "         29968, 29963, 22488,   508,   884,   367,  1304,   297,   916, 10161,\n",
    "           310,  7047,  5849, 29892,  1316,   408, 29901,    13,    13, 29930,\n",
    "           315,  9733, 13672, 20592,   848,   297,   263,  1856,  2280,   304,\n",
    "         10032,   278,  1353,   310,  2566,  9365, 29889,    13, 29930,   624,\n",
    "          8253, 15562, 29892,  1316,   408,  1404,  5821,  2063,   470,  2740,\n",
    "...\n",
    "           541,   508,   884,   367,  1304,   297,   916, 10161,   310,  7047,\n",
    "          5849,   304, 11157,  4180,   322,  8716,  3097, 29889,     2]],\n",
    "       device='cuda:2')\n",
    "With KV caching: 17.642 +- 0.0 seconds\n",
    "Output is truncated. View as a scrollable element or open in a text editor. Adjust cell output settings..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "model_path = \"/dataset/crosspipe/OriginModel/Llama-2-7b-chat-hf/\"\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, device_map=device)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.pad_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = [\n",
    "    \"please tell me What is KV caching?\",\n",
    "    \"what can i do to help you to do something?\"\n",
    "]\n",
    "inputs = tokenizer(input_texts,truncation=True,padding=True,max_length=256,return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['请告诉我 KV 缓存是什么？\\n\\nKV 缓存是一种基于内存的缓存系统，它通常用于存储大量的键值对，以提高系统的性能和可扩展性。KV 缓存通常用于存储快速访问的数据，例如用户 Session 信息、缓存、历史记录等。\\nKV 缓存的主要特点是：\\n1. 高性能：KV 缓存通常使用内存作为存储方式，因此它可以提供非常快的访问速度，比如读取和写入操作。\\n2. 低延迟：KV 缓存通常具有低延迟的特点，因为它可以直接从内存中读取和写入数据，而不需要进行磁盘IO等操作。\\n3. 高可扩展性：KV 缓存可以轻松地扩展，因为它可以通过添加更多的内存来提高性能。\\n4. 低成本：KV 缓存通常具有低成本的特点，因为它可以使用内存作为存储方式，而不需要购买和维护较昂贵的磁盘等设备。\\n5. 安全性：KV 缓存通常具有高度的安全性，因为它可以使用加密和权限控制来保护数据的安全性。\\n6. 可扩展性：KV 缓存可以轻松地扩展，因为它可以通过添加更多的内存来提高性能。\\n7. 可靠性：KV 缓存通常具有高度的可靠性，因为它可以通过多个内存节点来保证数据的可靠性。\\n\\n在实际应用中，KV 缓存通常用于各种场景，例如：\\n1. 缓存：KV 缓存可以用于缓存快速访问的数据，例如用户 Session 信息、缓存、历史记录等。\\n2. 数据存储：KV 缓存可以用于存储大量的键值对，例如数据库、缓存、历史记录等。\\n3. 消息队列：KV 缓存可以用于实现消息队列功能，例如在 Web 应用中实现消息队列功能。\\n4. 分布式系统：KV 缓存可以用于实现分布式系统的功能，例如在分布式文件系统中实现分布式文件系统的功能。\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', '我可以做些什么来帮助你？\\n\\nPlease let me know if you have any questions or if there is anything else I can do to help.\\n\\nI hope this helps!']\n"
     ]
    }
   ],
   "source": [
    "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**inputs, max_new_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['请告诉我 KV 缓存是什么？\\n\\nKV 缓存是一种基于内存的缓存系统，它通常用于存储大量的键值对，以提高系统的性能和可扩展性。KV 缓存通常用于存储快速访问的数据，例如用户 Session 信息、缓存、历史记录等。\\nKV 缓存的主要特点是：\\n1. 高性能：KV 缓存通常使用内存作为存储方式，因此它可以提供非常快的访问速度，比如读取和写入操作。\\n2. 低延迟：KV 缓存通常具有低延迟的特点，因为它可以直接从内存中读取和写入数据，而不需要进行磁盘IO等操作。\\n3. 高可扩展性：KV 缓存可以轻松地扩展，因为它可以通过添加更多的内存来提高性能。\\n4. 低成本：KV 缓存通常具有低成本的特点，因为它可以使用内存作为存储方式，而不需要购买和维护较昂贵的磁盘等设备。\\n5. 安全性：KV 缓存通常具有高度的安全性，因为它可以使用加密和权限控制来保护数据的安全性。\\n6. 可扩展性：KV 缓存可以轻松地扩展，因为它可以通过添加更多的内存来提高性能。\\n7. 可靠性：KV 缓存通常具有高度的可靠性，因为它可以通过多个内存节点来保证数据的可靠性。\\n\\n在实际应用中，KV 缓存通常用于各种场景，例如：\\n1. 缓存：KV 缓存可以用于缓存快速访问的数据，例如用户 Session 信息、缓存、历史记录等。\\n2. 数据存储：KV 缓存可以用于存储大量的键值对，例如数据库、缓存、历史记录等。\\n3. 消息队列：KV 缓存可以用于实现消息队列功能，例如在 Web 应用中实现消息队列功能。\\n4. 分布式系统：KV 缓存可以用于实现分布式系统的功能，例如在分布式文件系统中实现分布式文件系统的功能。\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n', '我可以做些什么来帮助你？\\n\\nPlease let me know if you have any questions or if there is anything else I can do to help.\\n\\nI hope this helps!']\n"
     ]
    }
   ],
   "source": [
    "input_texts = [\"请告诉我 KV 缓存是什么？\", \"我可以做些什么来帮助你？\"]\n",
    "inputs = tokenizer(input_texts, padding=True, truncation=True,max_length=256, return_tensors=\"pt\").to('cuda:2')\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# 使用模型生成文本\n",
    "generated_ids = model.generate(**inputs, max_new_tokens=1000)\n",
    "\n",
    "# 解码生成的 token ID\n",
    "decoded_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "\n",
    "print(decoded_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another way to decode❤\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = generated_ids.tolist()\n",
    "\n",
    "# 解码每一行\n",
    "decoded_texts = [tokenizer.decode(ids, skip_special_tokens=True) for ids in generated_ids]\n",
    "\n",
    "# 打印解码后的文本\n",
    "for text in decoded_texts:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "model_path = \"/dataset/crosspipe/OriginModel/Llama-2-7b-chat-hf/\"\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, device_map=device)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"What is KV caching ?\"\n",
    "inputs = tokenizer(input, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "int() argument must be a string, a bytes-like object or a real number, not 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3906\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, **kwargs)\u001b[0m\n\u001b[1;32m   3903\u001b[0m \u001b[38;5;66;03m# Convert inputs to python lists\u001b[39;00m\n\u001b[1;32m   3904\u001b[0m token_ids \u001b[38;5;241m=\u001b[39m to_py_obj(token_ids)\n\u001b[0;32m-> 3906\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3907\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3908\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclean_up_tokenization_spaces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3910\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3911\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:1072\u001b[0m, in \u001b[0;36mPreTrainedTokenizer._decode\u001b[0;34m(self, token_ids, skip_special_tokens, clean_up_tokenization_spaces, spaces_between_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_decode\u001b[39m(\n\u001b[1;32m   1063\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1064\u001b[0m     token_ids: List[\u001b[38;5;28mint\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1068\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   1069\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[1;32m   1070\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_decode_use_source_tokenizer \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_source_tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1072\u001b[0m     filtered_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_ids_to_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_special_tokens\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1073\u001b[0m     legacy_added_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_added_tokens_encoder\u001b[38;5;241m.\u001b[39mkeys()) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_tokens) \u001b[38;5;241m|\u001b[39m {\n\u001b[1;32m   1074\u001b[0m         token \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madditional_special_tokens \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconvert_tokens_to_ids(token) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size\n\u001b[1;32m   1075\u001b[0m     }\n\u001b[1;32m   1076\u001b[0m     \u001b[38;5;66;03m# To avoid mixing byte-level and unicode for byte-level BPT\u001b[39;00m\n\u001b[1;32m   1077\u001b[0m     \u001b[38;5;66;03m# we need to build string separately for added tokens and byte-level tokens\u001b[39;00m\n\u001b[1;32m   1078\u001b[0m     \u001b[38;5;66;03m# cf. https://github.com/huggingface/transformers/issues/1133\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils.py:1047\u001b[0m, in \u001b[0;36mPreTrainedTokenizer.convert_ids_to_tokens\u001b[0;34m(self, ids, skip_special_tokens)\u001b[0m\n\u001b[1;32m   1045\u001b[0m tokens \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m ids:\n\u001b[0;32m-> 1047\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1048\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m skip_special_tokens \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_special_ids:\n\u001b[1;32m   1049\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: int() argument must be a string, a bytes-like object or a real number, not 'list'"
     ]
    }
   ],
   "source": [
    "outputs = model.generate(**inputs)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
