{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "model_path = \"/dataset/crosspipe/OriginModel/Llama-2-7b-chat-hf/\"\n",
    " \n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, device_map=device)\n",
    "for use_cache in ( False,True):\n",
    "  times = []\n",
    "  for _ in range(1):  # measuring 10 generations\n",
    "      start = time.time()\n",
    "      input = tokenizer(\"What is KV caching?\", return_tensors=\"pt\").to(device)\n",
    "      outputs = model.generate(**input, use_cache=use_cache, max_new_tokens=1000, temperature=0.00001)\n",
    "      times.append(time.time() - start)\n",
    "      print(outputs)\n",
    "  print(f\"{'With' if use_cache else 'Without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tensor([[    1,  1724,   338,   476, 29963, 22488, 29973,    13,    13, 29968,\n",
    "         29963, 22488,   338,   263, 11043,  1304,   297,  7047,  5849,   304,\n",
    "         11157,   278,  4180,   310,  8324,   491, 15446, 13672, 20592,   848,\n",
    "           297,   263,  7090, 29889,   476, 29963, 15028,   363,   376,  1989,\n",
    "         29899,  1767,  1699,   607, 14637,   304,   278,  1134,   310,   848,\n",
    "          6087,   297,   278,  7090, 29889,    13,    13,   797,   263,   476,\n",
    "         29963,  7090, 29892,   848,   338,  6087,   408,   263,  5101,   310,\n",
    "           263,  1820,   322,   263,   995, 29889,   450,  1820,   338,  1304,\n",
    "           304, 12439,   278,   848, 29892,   322,   278,   995,   338,   278,\n",
    "           848,  3528, 29889,   450,  7090,   338, 12234,  8762,   408,   263,\n",
    "           848,  3829, 29892,  1316,   408,   263,  6608,  1591,   470,   263,\n",
    "          5447, 29892,   393,  6511,   363,  8543,  1106, 14340,   322, 11217,\n",
    "           310,   278,  6087,   848, 29889,    13,    13, 29968, 29963, 22488,\n",
    "           338, 15574,  1304,   297,  1856,  8324,   304, 11157,   278,  4180,\n",
    "           310,  2566,  9365, 29889,  8669,   310,  2346,   292,   278,  2566,\n",
    "           363,  1432,  2009, 29892,   278,  2280,   508,   671,   278,   476,\n",
    "         29963,  7090,   304,  3787, 13672, 20592,   848, 29892,  1316,   408,\n",
    "          1404,  4867,  2472,   470, 13672, 20592,  6515, 29889,   910, 26830,\n",
    "           278,  1353,   310,  2566,  9365, 29892,   607,   508, 11157,   278,\n",
    "          2280, 29915, 29879,  4180,   322,  8716,  3097, 29889,    13,    13,\n",
    "         29968, 29963, 22488,   508,   884,   367,  1304,   297,   916, 10161,\n",
    "           310,  7047,  5849, 29892,  1316,   408, 29901,    13,    13, 29930,\n",
    "           315,  9733, 13672, 20592,   848,   297,   263,  1856,  2280,   304,\n",
    "         10032,   278,  1353,   310,  2566,  9365, 29889,    13, 29930,   624,\n",
    "          8253, 15562, 29892,  1316,   408,  1404,  5821,  2063,   470,  2740,\n",
    "          4955, 29892,   297,   263,  7090,   304, 11157,   278,  4180,   310,\n",
    "           263,  1856,  2280, 29889,    13, 29930,   315,  9733, 13672, 20592,\n",
    "           848,   297,   263, 13235,  1788,   304, 10032,   278,  2254,   373,\n",
    "           278,  1788,   322, 11157,   967,  8716,  3097, 29889,    13,    13,\n",
    "           797, 15837, 29892,   476, 29963, 22488,   338,   263, 11043,  1304,\n",
    "           304, 11157,   278,  4180,   310,  8324,   491, 15446, 13672, 20592,\n",
    "           848,   297,   263,  7090, 29889,   739,   338, 15574,  1304,   297,\n",
    "          1856,  8324,   304, 10032,   278,  1353,   310,  2566,  9365, 29892,\n",
    "           541,   508,   884,   367,  1304,   297,   916, 10161,   310,  7047,\n",
    "          5849,   304, 11157,  4180,   322,  8716,  3097, 29889,     2]],\n",
    "       device='cuda:2')\n",
    "With KV caching: 18.327 +- 0.0 seconds\n",
    "tensor([[    1,  1724,   338,   476, 29963, 22488, 29973,    13,    13, 29968,\n",
    "         29963, 22488,   338,   263, 11043,  1304,   297,  7047,  5849,   304,\n",
    "         11157,   278,  4180,   310,  8324,   491, 15446, 13672, 20592,   848,\n",
    "           297,   263,  7090, 29889,   476, 29963, 15028,   363,   376,  1989,\n",
    "         29899,  1767,  1699,   607, 14637,   304,   278,  1134,   310,   848,\n",
    "          6087,   297,   278,  7090, 29889,    13,    13,   797,   263,   476,\n",
    "         29963,  7090, 29892,   848,   338,  6087,   408,   263,  5101,   310,\n",
    "           263,  1820,   322,   263,   995, 29889,   450,  1820,   338,  1304,\n",
    "           304, 12439,   278,   848, 29892,   322,   278,   995,   338,   278,\n",
    "           848,  3528, 29889,   450,  7090,   338, 12234,  8762,   408,   263,\n",
    "           848,  3829, 29892,  1316,   408,   263,  6608,  1591,   470,   263,\n",
    "          5447, 29892,   393,  6511,   363,  8543,  1106, 14340,   322, 11217,\n",
    "           310,   278,  6087,   848, 29889,    13,    13, 29968, 29963, 22488,\n",
    "           338, 15574,  1304,   297,  1856,  8324,   304, 11157,   278,  4180,\n",
    "           310,  2566,  9365, 29889,  8669,   310,  2346,   292,   278,  2566,\n",
    "           363,  1432,  2009, 29892,   278,  2280,   508,   671,   278,   476,\n",
    "         29963,  7090,   304,  3787, 13672, 20592,   848, 29892,  1316,   408,\n",
    "          1404,  4867,  2472,   470, 13672, 20592,  6515, 29889,   910, 26830,\n",
    "           278,  1353,   310,  2566,  9365, 29892,   607,   508, 11157,   278,\n",
    "          2280, 29915, 29879,  4180,   322,  8716,  3097, 29889,    13,    13,\n",
    "         29968, 29963, 22488,   508,   884,   367,  1304,   297,   916, 10161,\n",
    "           310,  7047,  5849, 29892,  1316,   408, 29901,    13,    13, 29930,\n",
    "           315,  9733, 13672, 20592,   848,   297,   263,  1856,  2280,   304,\n",
    "         10032,   278,  1353,   310,  2566,  9365, 29889,    13, 29930,   624,\n",
    "          8253, 15562, 29892,  1316,   408,  1404,  5821,  2063,   470,  2740,\n",
    "          4955, 29892,   297,   263,  7090,   304, 11157,   278,  4180,   310,\n",
    "           263,  1856,  2280, 29889,    13, 29930,   315,  9733, 13672, 20592,\n",
    "           848,   297,   263, 13235,  1788,   304, 10032,   278,  2254,   373,\n",
    "           278,  1788,   322, 11157,   967,  8716,  3097, 29889,    13,    13,\n",
    "           797, 15837, 29892,   476, 29963, 22488,   338,   263, 11043,  1304,\n",
    "           304, 11157,   278,  4180,   310,  8324,   491, 15446, 13672, 20592,\n",
    "           848,   297,   263,  7090, 29889,   739,   338, 15574,  1304,   297,\n",
    "          1856,  8324,   304, 10032,   278,  1353,   310,  2566,  9365, 29892,\n",
    "           541,   508,   884,   367,  1304,   297,   916, 10161,   310,  7047,\n",
    "          5849,   304, 11157,  4180,   322,  8716,  3097, 29889,     2]],\n",
    "       device='cuda:2')\n",
    "Without KV caching: 80.208 +- 0.0 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "model_path = \"/dataset/crosspipe/OriginModel/Llama-2-7b-chat-hf/\"\n",
    "device = \"cuda:1\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, device_map=device)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)\n",
    "input = \"What is KV caching ?\"\n",
    "inputs = tokenizer(input, return_tensors=\"pt\").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = dict(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs1 = model(**inputs)\n",
    "print(outputs1.past_key_values[0][0].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(outputs1[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for use_cache in (True, False):\n",
    "  times = []\n",
    "  for _ in range(1):  # measuring 10 generations\n",
    "      start = time.time()\n",
    "      input = tokenizer(\"What is KV caching?\", return_tensors=\"pt\").to(device)\n",
    "      outputs = model.generate(**input, use_cache=use_cache, max_new_tokens=1000, temperature=0.00001)\n",
    "      times.append(time.time() - start)\n",
    "      print(outputs)\n",
    "  print(f\"{'With' if use_cache else 'Without'} KV caching: {round(np.mean(times), 3)} +- {round(np.std(times), 3)} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
