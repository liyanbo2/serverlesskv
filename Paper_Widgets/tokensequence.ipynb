{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/anaconda3/envs/kvcache/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Union, Tuple, Optional\n",
    "from transformers import PreTrainedModel, PreTrainedTokenizer, PretrainedConfig\n",
    "import abc\n",
    "import torch\n",
    "from typing import Callable\n",
    "\n",
    "# Element is the base class for all elements in the prompt cache.\n",
    "# It defines the basic interface for all elements.\n",
    "class Element(ABC):\n",
    "    name: Union[None, str]\n",
    "    offset: int\n",
    "\n",
    "    def __init__(self, offset: int, name: Optional[str] = None):\n",
    "        self.name = name\n",
    "        self.offset = offset\n",
    "\n",
    "    @abstractmethod\n",
    "    def __len__(self) -> int:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def __repr__(self):\n",
    "        return f\"[{self.offset}:{self.offset + len(self)}]\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def token_ids(self) -> List[int]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abstractmethod\n",
    "    def position_ids(self) -> List[int]:\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class LanguageModel(ABC):\n",
    "    name: str\n",
    "    hf_tokenizer: PreTrainedTokenizer\n",
    "    hf_model: PreTrainedModel\n",
    "    stop_token_ids: List[int]\n",
    "    stop_str: List[str]\n",
    "    use_full_position_ids: bool = False\n",
    "\n",
    "    def __init__(self, name: str, model: PreTrainedModel, tokenizer: PreTrainedTokenizer,\n",
    "                 stop_token_ids: Optional[List[int]] = None, stop_str: Optional[List[str]] = None):\n",
    "        self.name = name\n",
    "        self.hf_tokenizer = tokenizer\n",
    "        self.hf_model = model\n",
    "        self.stop_token_ids = stop_token_ids if stop_token_ids is not None else [self.eos_token_id]\n",
    "        self.stop_str = stop_str if stop_str is not None else []\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def get_formatter(self) -> Callable[[str], str]:\n",
    "        pass\n",
    "\n",
    "    def get_cache_shape(self) -> Tuple[int, int, int]:\n",
    "        num_head = self.config.num_attention_heads\n",
    "        head_dim = self.config.hidden_size // self.config.num_attention_heads\n",
    "\n",
    "        return self.config.num_hidden_layers, num_head, head_dim\n",
    "\n",
    "    def store_k_hook(self, k_cache: torch.Tensor) -> torch.Tensor:\n",
    "        return k_cache\n",
    "\n",
    "    def store_v_hook(self, v_cache: torch.Tensor) -> torch.Tensor:\n",
    "        return v_cache\n",
    "\n",
    "    def read_k_hook(self, k_cache: torch.Tensor) -> torch.Tensor:\n",
    "        return k_cache\n",
    "\n",
    "    def read_v_hook(self, v_cache: torch.Tensor) -> torch.Tensor:\n",
    "        return v_cache\n",
    "\n",
    "    def __call__(self, **kwargs):\n",
    "        return self.hf_model(**kwargs)\n",
    "\n",
    "    def encode(self, text: str) -> List[int]:\n",
    "        # Warning: this is a hack to remove bos_token\n",
    "        token_ids = self.hf_tokenizer.encode(text, add_special_tokens=False)\n",
    "        return token_ids\n",
    "\n",
    "    def decode(self, token_ids: List[int]) -> str:\n",
    "        return self.hf_tokenizer.decode(token_ids, skip_special_tokens=False, spaces_between_special_tokens=False)\n",
    "\n",
    "    @property\n",
    "    def unk_token(self) -> str:\n",
    "        return self.hf_tokenizer.unk_token\n",
    "\n",
    "    @property\n",
    "    def unk_token_id(self) -> int:\n",
    "        return self.hf_tokenizer.unk_token_id\n",
    "\n",
    "    @property\n",
    "    def eos_token(self) -> str:\n",
    "        return self.hf_tokenizer.eos_token\n",
    "\n",
    "    @property\n",
    "    def eos_token_id(self) -> int:\n",
    "        return self.hf_tokenizer.eos_token_id\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        return self.hf_model.device\n",
    "\n",
    "    @property\n",
    "    def config(self) -> PretrainedConfig:\n",
    "        return self.hf_model.config\n",
    "\n",
    "class TokenSequence(Element):\n",
    "    text: str\n",
    "    token_ids: List[int]\n",
    "    position_ids: List[int]\n",
    "    \n",
    "    def __init__(self, offset: int, text: str, lm: LanguageModel, max_tokens: Optional[int] = None):\n",
    "        super().__init__(offset)\n",
    "        self.text = text\n",
    "        self.token_ids = lm.encode(text)\n",
    "        \n",
    "        if max_tokens is not None:\n",
    "            self._token_ids = self._token_ids[:max_tokens // 2] + self._token_ids[-max_tokens // 2:]\n",
    "        \n",
    "        self._position_ids = list(range(self.offset, self.offset + len(self._token_ids)))\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self.token_ids)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"TokenSequence({self.text})\"\n",
    "    \n",
    "    def token_ids(self) -> List[int]:\n",
    "        return self._token_ids\n",
    "    \n",
    "    def position_ids(self) -> List[int]:\n",
    "        return self._position_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TokenSequence(0, \"Hello, world!\", LanguageModel(\"gpt2\", None, None))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kvcache",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
