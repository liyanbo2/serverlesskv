{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a2246c929ad43538e9e3ab3d9006b99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "model_path = \"/dataset/crosspipe/OriginModel/Llama-2-7b-chat-hf/\"\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, device_map=device)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_Heavy_Hitter_FUNCTIONS = {\n",
    "    \"llama\": None,\n",
    "    \"llama_h2o\": H2OLlamaForCausalLM\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ENABLE_Heavy_Hitter_FUNCTIONS['llama_h2o'].from_pretrained(model_name, config=config,\n",
    "                                                                            cache_dir=args.cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class H2OLlamaForCausalLM(LlamaForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        num_layers = len(self.model.layers)\n",
    "        for layer_idx in range(num_layers):\n",
    "            self.model.layers[layer_idx].self_attn = H2OLlamaAttention(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.llama.configuration_llama import LlamaConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.configuration_utils import PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaConfig(PretrainedConfig):\n",
    "    r\"\"\"\n",
    "    This is the configuration class to store the configuration of a [`LlamaModel`]. It is used to instantiate an LLaMA\n",
    "    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n",
    "    defaults will yield a similar configuration to that of the LLaMA-7B.\n",
    "\n",
    "    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n",
    "    documentation from [`PretrainedConfig`] for more information.\n",
    "\n",
    "\n",
    "    Args:\n",
    "        vocab_size (`int`, *optional*, defaults to 32000):\n",
    "            Vocabulary size of the LLaMA model. Defines the number of different tokens that can be represented by the\n",
    "            `inputs_ids` passed when calling [`LlamaModel`]\n",
    "        hidden_size (`int`, *optional*, defaults to 4096):\n",
    "            Dimension of the hidden representations.\n",
    "        intermediate_size (`int`, *optional*, defaults to 11008):\n",
    "            Dimension of the MLP representations.\n",
    "        num_hidden_layers (`int`, *optional*, defaults to 32):\n",
    "            Number of hidden layers in the Transformer decoder.\n",
    "        num_attention_heads (`int`, *optional*, defaults to 32):\n",
    "            Number of attention heads for each attention layer in the Transformer decoder.\n",
    "        num_key_value_heads (`int`, *optional*):\n",
    "            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n",
    "            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n",
    "            `num_key_value_heads=1 the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n",
    "            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n",
    "            by meanpooling all the original heads within that group. For more details checkout [this\n",
    "            paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n",
    "            `num_attention_heads`.\n",
    "        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n",
    "            The non-linear activation function (function or string) in the decoder.\n",
    "        max_position_embeddings (`int`, *optional*, defaults to 2048):\n",
    "            The maximum sequence length that this model might ever be used with. Llama 1 supports up to 2048 tokens,\n",
    "            Llama 2 up to 4096, CodeLlama up to 16384.\n",
    "        initializer_range (`float`, *optional*, defaults to 0.02):\n",
    "            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n",
    "        rms_norm_eps (`float`, *optional*, defaults to 1e-06):\n",
    "            The epsilon used by the rms normalization layers.\n",
    "        use_cache (`bool`, *optional*, defaults to `True`):\n",
    "            Whether or not the model should return the last key/values attentions (not used by all models). Only\n",
    "            relevant if `config.is_decoder=True`.\n",
    "        pad_token_id (`int`, *optional*):\n",
    "            Padding token id.\n",
    "        bos_token_id (`int`, *optional*, defaults to 1):\n",
    "            Beginning of stream token id.\n",
    "        eos_token_id (`int`, *optional*, defaults to 2):\n",
    "            End of stream token id.\n",
    "        pretraining_tp (`int`, *optional*, defaults to 1):\n",
    "            Experimental feature. Tensor parallelism rank used during pretraining. Please refer to [this\n",
    "            document](https://huggingface.co/docs/transformers/parallelism) to understand more about it. This value is\n",
    "            necessary to ensure exact reproducibility of the pretraining results. Please refer to [this\n",
    "            issue](https://github.com/pytorch/pytorch/issues/76232).\n",
    "        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n",
    "            Whether to tie weight embeddings\n",
    "        rope_theta (`float`, *optional*, defaults to 10000.0):\n",
    "            The base period of the RoPE embeddings.\n",
    "        rope_scaling (`Dict`, *optional*):\n",
    "            Dictionary containing the scaling configuration for the RoPE embeddings. Currently supports two scaling\n",
    "            strategies: linear and dynamic. Their scaling factor must be a float greater than 1. The expected format is\n",
    "            `{\"type\": strategy name, \"factor\": scaling factor}`. When using this flag, don't update\n",
    "            `max_position_embeddings` to the expected new maximum. See the following thread for more information on how\n",
    "            these scaling strategies behave:\n",
    "            https://www.reddit.com/r/LocalLLaMA/comments/14mrgpr/dynamically_scaled_rope_further_increases/. This is an\n",
    "            experimental feature, subject to breaking API changes in future versions.\n",
    "        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n",
    "            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n",
    "        attention_dropout (`float`, *optional*, defaults to 0.0):\n",
    "            The dropout ratio for the attention probabilities.\n",
    "\n",
    "    ```python\n",
    "    >>> from transformers import LlamaModel, LlamaConfig\n",
    "\n",
    "    >>> # Initializing a LLaMA llama-7b style configuration\n",
    "    >>> configuration = LlamaConfig()\n",
    "\n",
    "    >>> # Initializing a model from the llama-7b style configuration\n",
    "    >>> model = LlamaModel(configuration)\n",
    "\n",
    "    >>> # Accessing the model configuration\n",
    "    >>> configuration = model.config\n",
    "    ```\"\"\"\n",
    "\n",
    "    model_type = \"llama\"\n",
    "    keys_to_ignore_at_inference = [\"past_key_values\"]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size=32000,\n",
    "        hidden_size=4096,\n",
    "        intermediate_size=11008,\n",
    "        num_hidden_layers=32,\n",
    "        num_attention_heads=32,\n",
    "        num_key_value_heads=None,\n",
    "        hidden_act=\"silu\",\n",
    "        max_position_embeddings=2048,\n",
    "        initializer_range=0.02,\n",
    "        rms_norm_eps=1e-6,\n",
    "        use_cache=True,\n",
    "        pad_token_id=None,\n",
    "        bos_token_id=1,\n",
    "        eos_token_id=2,\n",
    "        pretraining_tp=1,\n",
    "        tie_word_embeddings=False,\n",
    "        rope_theta=10000.0,\n",
    "        rope_scaling=None,\n",
    "        attention_bias=False,\n",
    "        attention_dropout=0.0,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_position_embeddings = max_position_embeddings\n",
    "        self.hidden_size = hidden_size\n",
    "        self.intermediate_size = intermediate_size\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.num_attention_heads = num_attention_heads\n",
    "\n",
    "        # for backward compatibility\n",
    "        if num_key_value_heads is None:\n",
    "            num_key_value_heads = num_attention_heads\n",
    "\n",
    "        self.num_key_value_heads = num_key_value_heads\n",
    "        self.hidden_act = hidden_act\n",
    "        self.initializer_range = initializer_range\n",
    "        self.rms_norm_eps = rms_norm_eps\n",
    "        self.pretraining_tp = pretraining_tp\n",
    "        self.use_cache = use_cache\n",
    "        self.rope_theta = rope_theta\n",
    "        self.rope_scaling = rope_scaling\n",
    "        self._rope_scaling_validation()\n",
    "        self.attention_bias = attention_bias\n",
    "        self.attention_dropout = attention_dropout\n",
    "\n",
    "        super().__init__(\n",
    "            pad_token_id=pad_token_id,\n",
    "            bos_token_id=bos_token_id,\n",
    "            eos_token_id=eos_token_id,\n",
    "            tie_word_embeddings=tie_word_embeddings,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "    def _rope_scaling_validation(self):\n",
    "        \"\"\"\n",
    "        Validate the `rope_scaling` configuration.\n",
    "        \"\"\"\n",
    "        if self.rope_scaling is None:\n",
    "            return\n",
    "\n",
    "        if not isinstance(self.rope_scaling, dict) or len(self.rope_scaling) != 2:\n",
    "            raise ValueError(\n",
    "                \"`rope_scaling` must be a dictionary with with two fields, `type` and `factor`, \"\n",
    "                f\"got {self.rope_scaling}\"\n",
    "            )\n",
    "        rope_scaling_type = self.rope_scaling.get(\"type\", None)\n",
    "        rope_scaling_factor = self.rope_scaling.get(\"factor\", None)\n",
    "        if rope_scaling_type is None or rope_scaling_type not in [\"linear\", \"dynamic\"]:\n",
    "            raise ValueError(\n",
    "                f\"`rope_scaling`'s type field must be one of ['linear', 'dynamic'], got {rope_scaling_type}\"\n",
    "            )\n",
    "        if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n",
    "            raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "\n",
    "class H2OLlamaAttention(nn.Module):\n",
    "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
    "\n",
    "    def __init__(self, config: LlamaConfig):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=False)\n",
    "        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=False)\n",
    "        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n",
    "        self._init_rope()\n",
    "\n",
    "        self.kv_cache = H2OKVCache_LayerWise(\n",
    "            hh_size=config.hh_size,\n",
    "            recent_size=config.recent_size,\n",
    "            k_seq_dim=2,\n",
    "            v_seq_dim=2,\n",
    "        )\n",
    "\n",
    "    def _init_rope(self):\n",
    "        if self.config.rope_scaling is None:\n",
    "            self.rotary_emb = LlamaRotaryEmbedding(\n",
    "                self.head_dim,\n",
    "                max_position_embeddings=self.max_position_embeddings,\n",
    "                base=self.rope_theta,\n",
    "            )\n",
    "        else:\n",
    "            scaling_type = self.config.rope_scaling[\"type\"]\n",
    "            scaling_factor = self.config.rope_scaling[\"factor\"]\n",
    "            if scaling_type == \"linear\":\n",
    "                self.rotary_emb = LlamaLinearScalingRotaryEmbedding(\n",
    "                    self.head_dim,\n",
    "                    max_position_embeddings=self.max_position_embeddings,\n",
    "                    scaling_factor=scaling_factor,\n",
    "                    base=self.rope_theta,\n",
    "                )\n",
    "            elif scaling_type == \"dynamic\":\n",
    "                self.rotary_emb = LlamaDynamicNTKScalingRotaryEmbedding(\n",
    "                    self.head_dim,\n",
    "                    max_position_embeddings=self.max_position_embeddings,\n",
    "                    scaling_factor=scaling_factor,\n",
    "                    base=self.rope_theta,\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n",
    "\n",
    "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
    "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
    "\n",
    "    def _clean_cache(self):\n",
    "        self.kv_cache._clean_scores()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
    "        output_attentions: bool = False,\n",
    "        use_cache: bool = False,\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "\n",
    "        bsz, q_len, _ = hidden_states.size()\n",
    "\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            key_value_slicing = (\n",
    "                self.num_key_value_heads * self.head_dim\n",
    "            ) // self.config.pretraining_tp\n",
    "            query_slices = self.q_proj.weight.split(\n",
    "                (self.num_heads * self.head_dim) // self.config.pretraining_tp, dim=0\n",
    "            )\n",
    "            key_slices = self.k_proj.weight.split(key_value_slicing, dim=0)\n",
    "            value_slices = self.v_proj.weight.split(key_value_slicing, dim=0)\n",
    "\n",
    "            query_states = [\n",
    "                F.linear(hidden_states, query_slices[i])\n",
    "                for i in range(self.config.pretraining_tp)\n",
    "            ]\n",
    "            query_states = torch.cat(query_states, dim=-1)\n",
    "\n",
    "            key_states = [\n",
    "                F.linear(hidden_states, key_slices[i])\n",
    "                for i in range(self.config.pretraining_tp)\n",
    "            ]\n",
    "            key_states = torch.cat(key_states, dim=-1)\n",
    "\n",
    "            value_states = [\n",
    "                F.linear(hidden_states, value_slices[i])\n",
    "                for i in range(self.config.pretraining_tp)\n",
    "            ]\n",
    "            value_states = torch.cat(value_states, dim=-1)\n",
    "\n",
    "        else:\n",
    "            query_states = self.q_proj(hidden_states)\n",
    "            key_states = self.k_proj(hidden_states)\n",
    "            value_states = self.v_proj(hidden_states)\n",
    "\n",
    "        query_states = query_states.view(\n",
    "            bsz, q_len, self.num_heads, self.head_dim\n",
    "        ).transpose(1, 2)\n",
    "        key_states = key_states.view(\n",
    "            bsz, q_len, self.num_key_value_heads, self.head_dim\n",
    "        ).transpose(1, 2)\n",
    "        value_states = value_states.view(\n",
    "            bsz, q_len, self.num_key_value_heads, self.head_dim\n",
    "        ).transpose(1, 2)\n",
    "\n",
    "        # remake causal mask\n",
    "        attention_mask = _make_causal_mask(\n",
    "            bsz=bsz,\n",
    "            tgt_len=q_len,\n",
    "            past_key_values_length=past_key_value[0].shape[-2] if past_key_value is not None else 0,\n",
    "            dtype=query_states.dtype,\n",
    "            device=query_states.device,\n",
    "        )\n",
    "\n",
    "        kv_seq_len = key_states.shape[-2]\n",
    "        if past_key_value is not None:\n",
    "            kv_seq_len += past_key_value[0].shape[-2]\n",
    "\n",
    "        position_length = kv_seq_len\n",
    "        if not position_ids.nelement() > 1:\n",
    "            if position_length < position_ids.item()+1:\n",
    "                position_length = position_ids.item()+1\n",
    "\n",
    "        cos, sin = self.rotary_emb(value_states, seq_len=position_length)\n",
    "        ### Shift Pos: query pos is min(cache_size, idx)\n",
    "        # query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n",
    "        query_states = apply_rotary_pos_emb_single(query_states, cos, sin, position_ids)\n",
    "        key_states = apply_rotary_pos_emb_single(key_states, cos, sin, position_ids)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # reuse k, v, self_attention\n",
    "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
    "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
    "\n",
    "        past_key_value = (key_states, value_states) if use_cache else None\n",
    "\n",
    "        # repeat k/v heads if n_kv_heads < n_heads\n",
    "        key_states = repeat_kv(key_states, self.num_key_value_groups)\n",
    "        value_states = repeat_kv(value_states, self.num_key_value_groups)\n",
    "\n",
    "        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(\n",
    "            self.head_dim\n",
    "        )\n",
    "\n",
    "        if attn_weights.size() != (bsz, self.num_heads, q_len, kv_seq_len):\n",
    "            raise ValueError(\n",
    "                f\"Attention weights should be of size {(bsz, self.num_heads, q_len, kv_seq_len)}, but is\"\n",
    "                f\" {attn_weights.size()}\"\n",
    "            )\n",
    "\n",
    "        if attention_mask is not None:\n",
    "            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n",
    "                raise ValueError(\n",
    "                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n",
    "                )\n",
    "            attn_weights = attn_weights + attention_mask\n",
    "\n",
    "        # upcast attention to fp32\n",
    "        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(\n",
    "            query_states.dtype\n",
    "        )\n",
    "\n",
    "        past_key_value = self.kv_cache(past_key_value, attn_weights.detach().clone())\n",
    "\n",
    "        attn_output = torch.matmul(attn_weights, value_states)\n",
    "\n",
    "        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n",
    "            raise ValueError(\n",
    "                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n",
    "                f\" {attn_output.size()}\"\n",
    "            )\n",
    "\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous()\n",
    "        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n",
    "\n",
    "        if self.config.pretraining_tp > 1:\n",
    "            attn_output = attn_output.split(\n",
    "                self.hidden_size // self.config.pretraining_tp, dim=2\n",
    "            )\n",
    "            o_proj_slices = self.o_proj.weight.split(\n",
    "                self.hidden_size // self.config.pretraining_tp, dim=1\n",
    "            )\n",
    "            attn_output = sum(\n",
    "                [\n",
    "                    F.linear(attn_output[i], o_proj_slices[i])\n",
    "                    for i in range(self.config.pretraining_tp)\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            attn_output = self.o_proj(attn_output)\n",
    "\n",
    "        if not output_attentions:\n",
    "            attn_weights = None\n",
    "\n",
    "        return attn_output, attn_weights, past_key_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
