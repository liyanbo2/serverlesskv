{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "model_path = \"/dataset/crosspipe/OriginModel/Llama-2-7b-chat-hf/\"\n",
    "device = \"cuda:2\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = LlamaForCausalLM.from_pretrained(model_path, device_map=device)\n",
    "tokenizer = LlamaTokenizer.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import AutoTokenizer\n",
    "# 加载模型和分词器\n",
    "model_path = \"/dataset/crosspipe/OriginModel/Llama-2-7b-chat-hf/\"\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(model_path).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "# 输入文本\n",
    "input_text = \"Once upon a time\"\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# 获取模型的输出，包括 hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(\n",
    "        inputs,\n",
    "        output_hidden_states=True,  # 启用 hidden states 的输出\n",
    "        return_dict=True  # 返回字典形式的输出，便于访问\n",
    "    )\n",
    "\n",
    "# 获取 hidden states\n",
    "hidden_states = outputs.hidden_states\n",
    "\n",
    "# 查看每一层的 hidden states 及其维度\n",
    "for i, layer_hidden_states in enumerate(hidden_states):\n",
    "    print(f\"Layer {i} hidden states shape: {layer_hidden_states.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_ids = inputs[:, :-1]\n",
    "next_word_ids = inputs[:, -1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs\n",
    "context_ids\n",
    "next_word_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_weights = outputs.attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if outputs.attentions is not None:\n",
    "    for layer_idx, layer_attention in enumerate(outputs.attentions):\n",
    "        print(f\"Layer {layer_idx}: {layer_attention[-1].shape}\")\n",
    "else:\n",
    "    print(\"Attention weights are None!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = outputs.sequences\n",
    "generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values = outputs.past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(past_key_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(past_key_values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_value1 = (\n",
    "    torch.tensor([[[[1, 2], [3, 4]], [[5, 6], [7, 8]]], [[[9, 10], [11, 12]], [[13, 14], [15, 16]]]]),\n",
    "    torch.tensor([[[[17, 18], [19, 20]], [[21, 22], [23, 24]]], [[[25, 26], [27, 28]], [[29, 30], [31, 32]]]])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key_states, value_states in past_key_value1:\n",
    "    \n",
    "    print(key_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for past_key, past_value in past_key_values:\n",
    "    print(f\"Key shape: {past_key.shape}\")\n",
    "    print(f\"Value shape: {past_value.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_key_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model.generate(input_ids=next_word_ids, past_key_values=past_key_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续生成文本\n",
    "input_text_continued = \"And they come across a huge monster\"\n",
    "inputs_continued = tokenizer(input_text_continued, return_tensors=\"pt\").to(device)\n",
    "inputs_continued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 继续生成文本并传递以前的 KV 缓存\n",
    "outputs_continued = model.generate(\n",
    "    **inputs_continued,\n",
    "    max_new_tokens=50,\n",
    "    use_cache=True,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,\n",
    "    past_key_values=past_key_values  # 传递以前的 KV 缓存\n",
    ")\n",
    "\n",
    "# 解码生成的文本\n",
    "generated_text = tokenizer.decode(outputs_continued.sequences[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"'Once upon a time\"\n",
    "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.generate(input_ids,  max_new_tokens=50, use_cache=True, return_dict_in_generate=True)\n",
    "past_key_values = out.past_key_values\n",
    "generated_ids = out.sequences\n",
    "\n",
    "# Now we can continue generation using cache and already generated tokens\n",
    "out_continued = model.generate(generated_ids,  max_new_tokens=50,past_key_values=past_key_values, return_dict_in_generate=True)\n",
    "continued_generated_ids = out_continued.sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_output = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n",
    "continued_output = tokenizer.batch_decode(continued_generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continued_output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
